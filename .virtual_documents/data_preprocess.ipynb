


import pandas as pd
import os

# Path to your folder
path = r"E:\patternrecognition\project\data"

# Load each file
df1 = pd.read_excel(os.path.join(path, "part1.xlsx"),header=None)
df2 = pd.read_csv(r"E:\patternrecognition\project\data\part2.txt",
                  sep=r"\s+",
                  header=None,
                  engine="python")
df3 = pd.read_excel(os.path.join(path, "part3.xlsx"),header=None)

# Combine vertically (stack)
df = pd.concat([df1, df2, df3], ignore_index=True)

# Separate labels and features
y = df.iloc[:, 0]      # first column → labels
X = df.iloc[:, 1:9]    # columns 2 to 9 → features

print("Shape of X:", X.shape)
print("Shape of y:", y.shape)



print("df1:", df1.shape)
print(df1.head())
print(df1.iloc[:, 0].isna().sum())
class_counts = df1.iloc[:, 0] .value_counts()
print(class_counts)


print("\ndf2:", df2.shape)
print(df2.head())
print(df2.iloc[:, 0].isna().sum())
class_counts = df2.iloc[:, 0] .value_counts()
print(class_counts)


print("\ndf3:", df3.shape)
print(df3.head())
print(df3.iloc[:, 0].isna().sum())
class_counts = df3.iloc[:, 0] .value_counts()
print(class_counts)


print("\ndf:", df.shape)
print(df.head())
print(df.iloc[:, 0].isna().sum())
class_counts = df.iloc[:, 0] .value_counts()
print(class_counts)



df.to_csv("combined_dataset.csv", index=False)


df.to_pickle("combined_dataset.pkl")





import pandas as pd
import numpy as np


df = pd.read_pickle("combined_dataset.pkl")


# Try converting all columns to numeric
df = df.iloc[1:, :].reset_index(drop=True)
df_numeric = df.apply(pd.to_numeric, errors='coerce')

# Rows where ANY column became NaN → invalid / bad row
bad_rows = df_numeric.isna().any(axis=1)

# Show them
print(df[bad_rows])
print("Number of corrupted rows:", bad_rows.sum())


df_clean = df_numeric[~bad_rows].reset_index(drop=True)
print("\ndf:", df_clean.shape)
print(df_clean.head())
print(df_clean.iloc[:, 0].isna().sum())
class_counts = df_clean.iloc[:, 0] .value_counts()
print(class_counts)


print("\ndf:", df_clean.shape)
print(df_clean.head())
print(df_clean.iloc[:, 0].isna().sum())
class_counts = df_clean.iloc[:, 0] .value_counts()
print(class_counts)


df_clean.to_pickle("cleaned_dataset.pkl")
df_clean.to_csv("cleaned_dataset.csv", index=False)


extra_duplicates = df_clean.duplicated().sum()
print("Extra duplicate rows:", extra_duplicates)



dups = df_clean[df_clean.duplicated(keep=False)]

dups = dups.astype(str).sort_values(list(df_clean.columns))
print(dups.shape)
print(dups.head(30))


df_nodup = df_clean.drop_duplicates(keep="first").reset_index(drop=True)


print("\ndf:", df_nodup.shape)
print(df_nodup.head())
print(df_nodup.iloc[:, 0].isna().sum())
class_counts = df_nodup.iloc[:, 0] .value_counts()
print(class_counts)


df_nodup.to_pickle("nodup_cleaned_dataset.pkl")
df_nodup.to_csv("nodup_cleaned_dataset.csv", index=False)





import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Global plotting style
plt.rcParams['font.family'] = 'Times New Roman'
plt.rcParams['figure.dpi'] = 300
plt.rcParams['axes.labelsize'] = 5
plt.rcParams['xtick.labelsize'] = 5
plt.rcParams['ytick.labelsize'] = 5
plt.rcParams['legend.fontsize'] = 5
plt.rcParams['axes.titlesize'] = 5
plt.rcParams['legend.fontsize'] = 5     
plt.rcParams['axes.grid'] = False    
sns.set_style("whitegrid")
sns.set_style(None)


df = pd.read_pickle("nodup_cleaned_dataset.pkl")
y = df.iloc[:, 0]
X = df.iloc[:, 1:9]

# Class counts
print("Shape of X:", X.shape)
print("Shape of y:", y.shape)
print(df.iloc[:, 0].isna().sum())

class_counts = y.value_counts()
print(class_counts)

plt.figure(figsize=(2,2))
sns.barplot(x=class_counts.index, y=class_counts.values)
plt.xlabel("Class")
plt.ylabel("Count")
plt.title("Class Distribution")
plt.tight_layout()
plt.grid(False)

plt.savefig("f1.png", dpi=300, bbox_inches='tight')  
plt.show()
df_corr = X.copy()
df_corr["label"] = y

corr = df_corr.corr()["label"].drop("label")  # correlation of each feature with class
print(corr)

plt.figure(figsize=(2,2))
sns.barplot(x=corr.index, y=corr.values)
plt.xticks(rotation=45)
plt.xlabel("Feature")
plt.ylabel("Correlation")
plt.title("Feature-Class Correlation")
plt.tight_layout()
plt.grid(False)
plt.savefig("f2.png", dpi=300, bbox_inches='tight') 
plt.show()


colors = y.map({-1: 'red', 1: 'blue'})

plt.figure(figsize=(3,2))
plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=colors, s=5, alpha=0.5)

plt.title("Feature 1 vs Feature 2")
plt.scatter([], [], color='red', label='Class -1', s=5)
plt.scatter([], [], color='blue', label='Class 1', s=5)

plt.xlabel("Feature 1")
plt.ylabel("Feature 2")

plt.legend(title="Classes", fontsize=4, title_fontsize=4)
plt.tight_layout()
plt.grid(False)
plt.savefig("f3.png", dpi=300, bbox_inches='tight')
plt.show()





from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# Standardize X
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

pca = PCA()
pca.fit(X_std)
PCs = pca.fit_transform(X_std)
explained_var = pca.explained_variance_ratio_
cum_explained = np.cumsum(explained_var)

plt.figure(figsize=(2,2))
plt.plot(range(1, len(explained_var)+1), explained_var, marker='o', linestyle='--', markersize=3)
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.title('Scree Plot')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.savefig("f4.png", dpi=300, bbox_inches='tight') 
plt.show()

plt.figure(figsize=(2,2))
plt.plot(range(1, len(cum_explained)+1), cum_explained, marker='o', linestyle='--', markersize=3 )
plt.xlabel('Principal Component')
plt.ylabel('Cumulative Explained Variance')
plt.title('Cumulative Explained Variance Plot')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.savefig("f5.png", dpi=300, bbox_inches='tight')
plt.show()  


# Convert y to numpy array
#y_np = y.to_numpy().astype(float)

# Initialize list to store correlations
corr_list = []

# Loop through each PCA component
for i in range(PCs.shape[1]):
    pc = PCs[:, i]
    corr = np.corrcoef(pc,y)[0, 1]
    corr_list.append(corr)


#print(corr_list)

plt.figure(figsize=(2,2))
plt.bar(np.arange(1, len(corr_list)+1), corr_list)
plt.xlabel("PCA Component")
plt.ylabel("Correlation with Label")
plt.title("PCA–Label Correlation")
plt.tight_layout()
plt.grid(False)
plt.savefig("f6.png", dpi=300, bbox_inches='tight')
plt.show()


colors = y.map({-1: 'red', 1: 'blue'})

plt.figure(figsize=(3,2))
plt.scatter(PCs[:, 3], PCs[:, 5], c=colors, s=5, alpha=0.5)

plt.title("PC4 vs PC6")
plt.scatter([], [], color='red', label='Class -1', s=5)
plt.scatter([], [], color='blue', label='Class 1', s=5)

plt.xlabel("PC4")
plt.ylabel("PC6")

plt.legend(title="Classes", fontsize=4, title_fontsize=4)
plt.tight_layout()
plt.grid(False)
plt.savefig("f7.png", dpi=300, bbox_inches='tight')
plt.show()


plt.figure(figsize=(3,2))
plt.scatter(PCs[:, 3], PCs[:, 4], c=colors, s=5, alpha=0.5)

plt.title("PC4 vs PC5")
plt.scatter([], [], color='red', label='Class -1', s=5)
plt.scatter([], [], color='blue', label='Class 1', s=5)

plt.xlabel("PC4")
plt.ylabel("PC5")

plt.legend(title="Classes", fontsize=4, title_fontsize=4)
plt.tight_layout()
plt.grid(False)
plt.savefig("f8.png", dpi=300, bbox_inches='tight')
plt.show()





import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pickle

X_np = np.asarray(X)   # shape (N, 8)
y_np = np.asarray(y)   # shape (N,)
print(X_np.shape)
print(y_np.shape)

scaler = StandardScaler()
X_norm = scaler.fit_transform(X_np)   # still numpy array


# Train (70%) vs Test (30%)

X_train, X_test, y_train, y_test = train_test_split(
    X_norm, y_np, test_size=0.30, shuffle=True, random_state=42
)


print("Train:", X_train.shape, y_train.shape)
print("Test:", X_test.shape, y_test.shape)


with open("X_train.pkl", "wb") as f: pickle.dump(X_train, f)
with open("y_train.pkl", "wb") as f: pickle.dump(y_train, f)

with open("X_test.pkl", "wb") as f: pickle.dump(X_test, f)
with open("y_test.pkl", "wb") as f: pickle.dump(y_test, f)


