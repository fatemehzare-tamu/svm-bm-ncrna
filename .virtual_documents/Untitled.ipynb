


import pandas as pd
import os

# Path to your folder
path = r"E:\patternrecognition\project\data"

# Load each file
df1 = pd.read_excel(os.path.join(path, "part1.xlsx"),header=None)
df2 = pd.read_csv(r"E:\patternrecognition\project\data\part2.txt",
                  sep=r"\s+",
                  header=None,
                  engine="python")
df3 = pd.read_excel(os.path.join(path, "part3.xlsx"),header=None)

# Combine vertically (stack)
df = pd.concat([df1, df2, df3], ignore_index=True)

# Separate labels and features
y = df.iloc[:, 0]      # first column → labels
X = df.iloc[:, 1:9]    # columns 2 to 9 → features

print("Shape of X:", X.shape)
print("Shape of y:", y.shape)



print("df1:", df1.shape)
print(df1.head())
print(df1.iloc[:, 0].isna().sum())
class_counts = df1.iloc[:, 0] .value_counts()
print(class_counts)


print("\ndf2:", df2.shape)
print(df2.head())
print(df2.iloc[:, 0].isna().sum())
class_counts = df2.iloc[:, 0] .value_counts()
print(class_counts)


print("\ndf3:", df3.shape)
print(df3.head())
print(df3.iloc[:, 0].isna().sum())
class_counts = df3.iloc[:, 0] .value_counts()
print(class_counts)


print("\ndf:", df.shape)
print(df.head())
print(df.iloc[:, 0].isna().sum())
class_counts = df.iloc[:, 0] .value_counts()
print(class_counts)



df.to_csv("combined_dataset.csv", index=False)


df.to_pickle("combined_dataset.pkl")





import pandas as pd
import numpy as np


df = pd.read_pickle("combined_dataset.pkl")


# Try converting all columns to numeric
df = df.iloc[1:, :].reset_index(drop=True)
df_numeric = df.apply(pd.to_numeric, errors='coerce')

# Rows where ANY column became NaN → invalid / bad row
bad_rows = df_numeric.isna().any(axis=1)

# Show them
print(df[bad_rows])
print("Number of corrupted rows:", bad_rows.sum())


df_clean = df_numeric[~bad_rows].reset_index(drop=True)
print("\ndf:", df_clean.shape)
print(df_clean.head())
print(df_clean.iloc[:, 0].isna().sum())
class_counts = df_clean.iloc[:, 0] .value_counts()
print(class_counts)


print("\ndf:", df_clean.shape)
print(df_clean.head())
print(df_clean.iloc[:, 0].isna().sum())
class_counts = df_clean.iloc[:, 0] .value_counts()
print(class_counts)


df_clean.to_pickle("cleaned_dataset.pkl")
df_clean.to_csv("cleaned_dataset.csv", index=False)


extra_duplicates = df_clean.duplicated().sum()
print("Extra duplicate rows:", extra_duplicates)



dups = df_clean[df_clean.duplicated(keep=False)]

dups = dups.astype(str).sort_values(list(df_clean.columns))
print(dups.shape)
print(dups.head(30))


df_nodup = df_clean.drop_duplicates(keep="first").reset_index(drop=True)


print("\ndf:", df_nodup.shape)
print(df_nodup.head())
print(df_nodup.iloc[:, 0].isna().sum())
class_counts = df_nodup.iloc[:, 0] .value_counts()
print(class_counts)


df_nodup.to_pickle("nodup_cleaned_dataset.pkl")
df_nodup.to_csv("nodup_cleaned_dataset.csv", index=False)





import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Global plotting style
plt.rcParams['font.family'] = 'Times New Roman'
plt.rcParams['figure.dpi'] = 300
plt.rcParams['axes.labelsize'] = 5
plt.rcParams['xtick.labelsize'] = 5
plt.rcParams['ytick.labelsize'] = 5
plt.rcParams['legend.fontsize'] = 5
plt.rcParams['axes.titlesize'] = 5
plt.rcParams['legend.fontsize'] = 5     
plt.rcParams['axes.grid'] = False    
sns.set_style("whitegrid")
sns.set_style(None)


df = pd.read_pickle("nodup_cleaned_dataset.pkl")
y = df.iloc[:, 0]
X = df.iloc[:, 1:9]

# Class counts
print("Shape of X:", X.shape)
print("Shape of y:", y.shape)
print(df.iloc[:, 0].isna().sum())

class_counts = y.value_counts()
print(class_counts)

plt.figure(figsize=(2,2))
sns.barplot(x=class_counts.index, y=class_counts.values)
plt.xlabel("Class")
plt.ylabel("Count")
plt.title("Class Distribution")
plt.tight_layout()
plt.grid(False)

plt.savefig("f1.png", dpi=300, bbox_inches='tight')  
plt.show()
df_corr = X.copy()
df_corr["label"] = y

corr = df_corr.corr()["label"].drop("label")  # correlation of each feature with class
print(corr)

plt.figure(figsize=(2,2))
sns.barplot(x=corr.index, y=corr.values)
plt.xticks(rotation=45)
plt.xlabel("Feature")
plt.ylabel("Correlation")
plt.title("Feature-Class Correlation")
plt.tight_layout()
plt.grid(False)
plt.savefig("f2.png", dpi=300, bbox_inches='tight') 
plt.show()


colors = y.map({-1: 'red', 1: 'blue'})

plt.figure(figsize=(3,2))
plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=colors, s=5, alpha=0.5)

plt.title("Feature 1 vs Feature 2")
plt.scatter([], [], color='red', label='Class -1', s=5)
plt.scatter([], [], color='blue', label='Class 1', s=5)

plt.xlabel("Feature 1")
plt.ylabel("Feature 2")

plt.legend(title="Classes", fontsize=4, title_fontsize=4)
plt.tight_layout()
plt.grid(False)
plt.savefig("f3.png", dpi=300, bbox_inches='tight')
plt.show()





from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# Standardize X
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

pca = PCA()
pca.fit(X_std)
PCs = pca.fit_transform(X_std)
explained_var = pca.explained_variance_ratio_
cum_explained = np.cumsum(explained_var)

plt.figure(figsize=(2,2))
plt.plot(range(1, len(explained_var)+1), explained_var, marker='o', linestyle='--', markersize=3)
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.title('Scree Plot')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.savefig("f4.png", dpi=300, bbox_inches='tight') 
plt.show()

plt.figure(figsize=(2,2))
plt.plot(range(1, len(cum_explained)+1), cum_explained, marker='o', linestyle='--', markersize=3 )
plt.xlabel('Principal Component')
plt.ylabel('Cumulative Explained Variance')
plt.title('Cumulative Explained Variance Plot')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.savefig("f5.png", dpi=300, bbox_inches='tight')
plt.show()  


# Convert y to numpy array
#y_np = y.to_numpy().astype(float)

# Initialize list to store correlations
corr_list = []

# Loop through each PCA component
for i in range(PCs.shape[1]):
    pc = PCs[:, i]
    corr = np.corrcoef(pc,y)[0, 1]
    corr_list.append(corr)


#print(corr_list)

plt.figure(figsize=(2,2))
plt.bar(np.arange(1, len(corr_list)+1), corr_list)
plt.xlabel("PCA Component")
plt.ylabel("Correlation with Label")
plt.title("PCA–Label Correlation")
plt.tight_layout()
plt.grid(False)
plt.savefig("f6.png", dpi=300, bbox_inches='tight')
plt.show()


colors = y.map({-1: 'red', 1: 'blue'})

plt.figure(figsize=(3,2))
plt.scatter(PCs[:, 3], PCs[:, 5], c=colors, s=5, alpha=0.5)

plt.title("PC4 vs PC6")
plt.scatter([], [], color='red', label='Class -1', s=5)
plt.scatter([], [], color='blue', label='Class 1', s=5)

plt.xlabel("PC4")
plt.ylabel("PC6")

plt.legend(title="Classes", fontsize=4, title_fontsize=4)
plt.tight_layout()
plt.grid(False)
plt.savefig("f7.png", dpi=300, bbox_inches='tight')
plt.show()


plt.figure(figsize=(3,2))
plt.scatter(PCs[:, 3], PCs[:, 4], c=colors, s=5, alpha=0.5)

plt.title("PC4 vs PC5")
plt.scatter([], [], color='red', label='Class -1', s=5)
plt.scatter([], [], color='blue', label='Class 1', s=5)

plt.xlabel("PC4")
plt.ylabel("PC5")

plt.legend(title="Classes", fontsize=4, title_fontsize=4)
plt.tight_layout()
plt.grid(False)
plt.savefig("f8.png", dpi=300, bbox_inches='tight')
plt.show()





import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pickle

X_np = np.asarray(X)   # shape (N, 8)
y_np = np.asarray(y)   # shape (N,)
print(X_np.shape)
print(y_np.shape)

scaler = StandardScaler()
X_norm = scaler.fit_transform(X_np)   # still numpy array


# Train (70%) vs Test (30%)

X_train, X_test, y_train, y_test = train_test_split(
    X_norm, y_np, test_size=0.30, shuffle=True, random_state=42
)


print("Train:", X_train.shape, y_train.shape)
print("Test:", X_test.shape, y_test.shape)


with open("X_train.pkl", "wb") as f: pickle.dump(X_train, f)
with open("y_train.pkl", "wb") as f: pickle.dump(y_train, f)

with open("X_test.pkl", "wb") as f: pickle.dump(X_test, f)
with open("y_test.pkl", "wb") as f: pickle.dump(y_test, f)







import pickle

# Load X arrays
with open("X_train.pkl", "rb") as f:
    X_train = pickle.load(f)

with open("X_test.pkl", "rb") as f:
    X_test = pickle.load(f)

# Load y arrays
with open("y_train.pkl", "rb") as f:
    y_train = pickle.load(f)

with open("y_test.pkl", "rb") as f:
    y_test = pickle.load(f)



from sklearn.model_selection import train_test_split

# Shuffle + keep 10%
X_train_small, _, y_train_small, _ = train_test_split(
    X_train, y_train,
    test_size=0.9,         # Drop 90%, keep 10%
    shuffle=True,
    random_state=42        # For reproducibility
)



# Shuffle + keep 10%
X_test_small, _, y_test_small, _ = train_test_split(
    X_test, y_test,
    test_size=0.9,         # Drop 90%, keep 10%
    shuffle=True,
    random_state=42        # For reproducibility
)



from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import time
from sklearn.linear_model import SGDClassifier
import numpy as np


#X_train = X_train[:, :2]   # columns 0 and 1
#X_test  = X_test[:, :2]

X_train = X_train_small
y_train = y_train_small
X_test = X_test_small
y_test = y_test_small

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

classes, counts = np.unique(y_train, return_counts=True)
for c, cnt in zip(classes, counts):
    print(f"Train Class {c}: {cnt} samples")

classes, counts = np.unique(y_test, return_counts=True)
for c, cnt in zip(classes, counts):
    print(f"Test Class {c}: {cnt} samples")

# Train
t0 = time.time()

#svm_sgd = SGDClassifier(loss="hinge", max_iter=1000, verbose=1)
#svm_sgd.fit(X_train, y_train)


# SVM model (you can tune later)
svm = SVC(kernel='rbf', C=1.0, gamma='scale', verbose=True)
svm.fit(X_train, y_train)


t1 = time.time()
print("Training time (seconds):", t1 - t0)



y_pred_train = svm.predict(X_train)
y_pred_test  = svm.predict(X_test)

print("Train Accuracy:", accuracy_score(y_train, y_pred_train))
print("Test Accuracy:", accuracy_score(y_test, y_pred_test))

cm_train = confusion_matrix(y_train, y_pred_train)
cm_test  = confusion_matrix(y_test, y_pred_test)

print("\nConfusion Matrix (Train):\n", cm_train)
print("\nConfusion Matrix (Test):\n", cm_test)

print("\nClassification Report (Test):")
print(classification_report(y_test, y_pred_test))



from cuml.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import time

# IMPORTANT: convert to CuPy or GPU DataFrame
import cupy as cp

X_train_gpu = cp.asarray(X_train[:, :2])
X_test_gpu  = cp.asarray(X_test[:, :2])
y_train_gpu = cp.asarray(y_train)
y_test_gpu  = cp.asarray(y_test)

# Train on GPU
t0 = time.time()

svm = SVC(kernel='rbf', C=1.0, gamma='scale')
svm.fit(X_train_gpu, y_train_gpu)

t1 = time.time()
print("Training time (seconds):", t1 - t0)

# Predict
y_pred_gpu = svm.predict(X_test_gpu)

# Move back to CPU for sklearn metrics
y_pred = cp.asnumpy(y_pred_gpu)
y_test_cpu = cp.asnumpy(y_test_gpu)

print("Accuracy:", accuracy_score(y_test_cpu, y_pred))
print(confusion_matrix(y_test_cpu, y_pred))
print(classification_report(y_test_cpu, y_pred))



conda install -c rapidsai -c nvidia -c conda-forge \
    cuml=23.08 python=3.10 cudatoolkit=11.8



conda create -n env-gpu python=3.10


from thundersvm import SVC
import time

t0 = time.time()

model = SVC(kernel='rbf', C=1.0, gamma='scale')
model.fit(X_train, y_train)

t1 = time.time()

print("Training time:", t1 - t0)




